# Model configuration
model:
  path: null                            # Path to the directory containing models
  name: null                            # Name of the model class
  weights: null                         # Path to the model weights. Use checkpoint_dir to load weights if training
  args: null                            # Model arguments
  kwargs: null                          # Model keyword arguments
   
# Dataset configuration
dataset:
  data_path: null                       # Path dataset containing train/ and test/ directories
  size_train: 1                         # Number of files to sample and train on
  size_test: 1                          # Number of files to sample and test on.
  num_processes: 1                      # Number of threads for data loading
  num_workers: 1                        # Number of workers for data loading
  shuffle_buffer: 10_000                # Buffer size for shuffling the HF streaming dataset

# Training configuration
train:
  rounds: 1                             # Number of times to sample from dataset
  epochs: 1                             # Number of epochs to train on sampled data
  batch_size: 1                         # Batch size (test/train)
  validation_every: 0                   # Validate the model every N iterations
  lr: 0.0005                            # Learning rate
  min_lr: 0.00005                       # Minimum learning rate
  scheduler: cosine                     # Learning rate scheduler ('linear' or 'cosine')
  warmup_lr: 0.00005                    # Initial learning rate during warmup
  warmup_iters: 0                       # Number of iterations going from warmup_lr to lr
  warmup_strategy: cos                  # Warmup strategy
  scheduler_iters: 100000               # Total scheduler steps (in iterations)
  optimizer: adamw                      # Optimizer. Any of the names in timm.optim.list_optimizers()
  compile: false                        # Compile the model. Uses INDUCTOR
  amp: 'no'                             # automatic mixed precision: 'no','fp16','bf16' or 'fp8'
  grad_clip: 1.0                        # Gradient clipping norm value.
  grad_accum: 0                         # Gradient accumulation steps. Greater than 0 to enable
  device: cuda                          # Device to train on
  output_dir: null                      # Output directory for training artifacts
  checkpoint_dir: null                  # Checkpoint directory to load from previous training
  resume_from_checkpoint: false         # Will reuse experiment directory

# Logging configuration
logging:
  log_every: 200                        # Log metrics every N iterations
  wandb: false                          # Enable logging to Weights & Biases
  wandb_project: chessbot               # W&B project name
  wandb_run_name: null                  # W&B run name. Randomly generated if null
  wandb_run_id: null                    # W&B run ID used to resume training

